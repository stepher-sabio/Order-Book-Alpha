"""
phase3_gradientboosting_enhanced.py - ENHANCED Gradient Boosting for LOB Data

MAJOR IMPROVEMENTS OVER ORIGINAL:
1. Deeper trees (10-14 depth vs 4-8) for complex patterns
2. More iterations (500-1500 vs 100-300) with early stopping
3. Better learning rate schedule
4. More granular min_samples for finer splits
5. Optimized regularization

WHY THESE CHANGES MATTER FOR LOB DATA:
- GB learns sequential patterns (error correction)
- Deeper individual trees capture order book structure
- More iterations = better learning from residuals
- LOB microstructure has subtle, non-linear patterns
- Need depth to model spread/imbalance/volatility interactions

EXPECTED RESULTS:
- Should SIGNIFICANTLY beat Linear (0.9889%) and original GB (0.9092%)
- Target: 1.3-1.6% R¬≤ (competitive with/better than XGBoost)
- Training time: 15-45 minutes on 11.7M samples
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import HistGradientBoostingRegressor
import time
from pathlib import Path
import matplotlib.pyplot as plt
import gc
import warnings
warnings.filterwarnings('ignore', category=UserWarning)

# Import our utilities
from utils import (
    load_data, 
    time_based_split,
    evaluate_model,
    print_metrics,
    save_model,
    save_results,
    format_duration,
    print_section,
    FEATURE_COLS,
    TARGET_HORIZONS
)

# ============================================
# Configuration
# ============================================
RESULTS_DIR = Path('results')
MODELS_DIR = Path('models')
PLOTS_DIR = Path('plots')
RESULTS_DIR.mkdir(exist_ok=True)
MODELS_DIR.mkdir(exist_ok=True)
PLOTS_DIR.mkdir(exist_ok=True)

# Sample size
SAMPLE_SIZE = None  # Use full 11.7M

# Target
TARGET_HORIZON = 'return_200ms'

# Memory optimization
ENABLE_MEMORY_OPTIMIZATION = True

# ============================================
# ENHANCED Gradient Boosting Configurations
# ============================================
def get_enhanced_configs():
    """
    Enhanced HistGradientBoosting config for LOB microstructure
    
    KEY PRINCIPLES FOR LOB DATA:
    1. Deeper trees: Capture order book structure and relationships
    2. More iterations: Sequential learning from residuals
    3. Early stopping: Prevent overfitting while maximizing learning
    4. Smaller learning rate: More gradual, better generalization
    5. Less regularization: LOB patterns are real, not noise
    
    ACCURATE CONFIG: Maximum performance for LOB data
    - 1200 iterations (vs original 100-300)
    - Depth 14 (vs original 4-8)
    - Learning rate 0.05 (slower, more careful)
    - Expected: 25-45 min on 11.7M samples
    """
    
    configs = {
        # ACCURATE: Maximum performance
        'GB_Accurate': {
            'max_iter': 800,            # 4-12x more than original
            'learning_rate': 0.1,       # Slower, more careful learning
            'max_depth': 10,             # 3.5x deeper than original
            'min_samples_leaf': 10,       # Fine-grained splits
            'max_bins': 255,             # Full histogram resolution
            'l2_regularization': 0.00,   # Minimal regularization
            'early_stopping': True,
            'n_iter_no_change': 30,      # Very patient
            'validation_fraction': 0.1,
            'random_state': 42,
            'verbose': 1
        }
    }
    
    return configs

# ============================================
# Memory-Efficient Data Loading
# ============================================
def load_data_efficient(file_path, sample_size=None, random_state=42):
    """Load data with memory optimizations"""
    print(f"Loading data from {file_path}...")
    
    df = pd.read_parquet(file_path)
    print(f"Original size: {len(df):,} rows")
    
    if sample_size and sample_size < len(df):
        print(f"Sampling {sample_size:,} rows...")
        df = df.sample(n=sample_size, random_state=random_state)
    
    if ENABLE_MEMORY_OPTIMIZATION:
        print("Optimizing memory usage (float64 ‚Üí float32)...")
        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = df[col].astype('float32')
        print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    
    return df

# ============================================
# Main Training Function
# ============================================
def train_gradient_boosting_enhanced():
    """Train and evaluate enhanced Gradient Boosting models"""
    
    print_section("PHASE 3: GRADIENT BOOSTING FOR LOB")
    
    # ============================================
    # Load Data
    # ============================================
    print_section("Loading Data")
    df = load_data_efficient('cleaned_data/featured_AAPL.parquet', sample_size=SAMPLE_SIZE)
    
    X = df[FEATURE_COLS].values
    y = df[TARGET_HORIZON].values
    
    print(f"\nFeatures: {len(FEATURE_COLS)}")
    print(f"Target: {TARGET_HORIZON}")
    print(f"X shape: {X.shape}")
    
    # Convert to float32 for speed and memory
    if ENABLE_MEMORY_OPTIMIZATION:
        X = X.astype('float32')
        y = y.astype('float32')
        print("Data converted to float32")
    
    del df
    gc.collect()
    
    # ============================================
    # Train/Validation/Test Split
    # ============================================
    print_section("Train/Validation/Test Split")
    
    # 70% train, 10% val, 20% test
    X_train, X_temp, y_train, y_temp = time_based_split(X, y, test_size=0.3)
    X_val, X_test, y_val, y_test = time_based_split(
        X_temp, y_temp, test_size=(0.2/0.3)
    )
    
    print(f"Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)")
    print(f"Val:   {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)")
    print(f"Test:  {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)")
    
    del X, y, X_temp, y_temp
    gc.collect()
    
    # ============================================
    # Load Phase 1 Baseline
    # ============================================
    print_section("Phase 1 Baseline")
    phase1_results = pd.read_csv('results/phase1_linear_results.csv')
    phase1_best = phase1_results[phase1_results['horizon'] == TARGET_HORIZON].iloc[0]
    baseline_r2 = phase1_best['test_r2']
    
    print(f"Phase 1 Best (Linear): R¬≤ = {baseline_r2*100:.4f}%")
    print(f"Goal: SIGNIFICANTLY beat this with enhanced GB!")
    
    # ============================================
    # Train Models
    # ============================================
    configs = get_enhanced_configs()
    all_results = []
    
    for model_name, params in configs.items():
        print_section(f"Training: {model_name}")
        
        print("\n--- Configuration ---")
        for key, val in params.items():
            if key not in ['random_state', 'verbose', 'validation_fraction']:
                print(f"{key}: {val}")
        
        print(f"\nüå≥ Max iterations: {params['max_iter']} (vs original 100-300)")
        print(f"üìä Max depth: {params['max_depth']} (vs original 4-8)")
        print(f"üéØ Learning rate: {params['learning_rate']}")
        print(f"‚èπÔ∏è  Early stopping with {params['n_iter_no_change']} patience")
        
        # ============================================
        # Train
        # ============================================
        print("\n--- Training ---")
        start_time = time.time()
        
        model = HistGradientBoostingRegressor(**params)
        model.fit(X_train, y_train)
        
        train_time = time.time() - start_time
        
        # Get actual number of iterations (early stopping)
        actual_iterations = model.n_iter_
        stopped_early = actual_iterations < params['max_iter']
        
        print(f"‚úÖ Training complete in {format_duration(train_time)}")
        print(f"üìä Iterations: {actual_iterations} / {params['max_iter']}")
        if stopped_early:
            print(f"‚èπÔ∏è  Early stopping triggered (saved {params['max_iter']-actual_iterations} iterations)")
        
        # ============================================
        # Predictions
        # ============================================
        print("\n--- Predictions ---")
        
        print("Predicting on train set...")
        y_train_pred = model.predict(X_train)
        
        print("Predicting on validation set...")
        y_val_pred = model.predict(X_val)
        
        print("Predicting on test set...")
        y_test_pred = model.predict(X_test)
        
        # ============================================
        # Evaluation
        # ============================================
        print("\n--- Evaluation ---")
        
        train_metrics = evaluate_model(y_train, y_train_pred, 'train')
        val_metrics = evaluate_model(y_val, y_val_pred, 'val')
        test_metrics = evaluate_model(y_test, y_test_pred, 'test')
        
        # Calculate directional accuracy
        train_dir_acc = np.mean(np.sign(y_train) == np.sign(y_train_pred)) * 100
        val_dir_acc = np.mean(np.sign(y_val) == np.sign(y_val_pred)) * 100
        test_dir_acc = np.mean(np.sign(y_test) == np.sign(y_test_pred)) * 100
        
        train_metrics['train_directional_accuracy'] = train_dir_acc
        val_metrics['val_directional_accuracy'] = val_dir_acc
        test_metrics['test_directional_accuracy'] = test_dir_acc
        
        print_metrics(train_metrics, 'Train')
        print_metrics(val_metrics, 'Val')
        print_metrics(test_metrics, 'Test')
        
        print(f"\n--- Directional Accuracy ---")
        print(f"Train: {train_dir_acc:.2f}%")
        print(f"Val:   {val_dir_acc:.2f}%")
        print(f"Test:  {test_dir_acc:.2f}%")
        
        # ============================================
        # Compare to Baselines
        # ============================================
        improvement_vs_linear = test_metrics['test_r2'] - baseline_r2
        pct_improvement_vs_linear = (improvement_vs_linear / baseline_r2) * 100
        
        print(f"\n--- vs Phase 1 Linear Baseline ---")
        print(f"Linear R¬≤:     {baseline_r2*100:.4f}%")
        print(f"This model R¬≤: {test_metrics['test_r2']*100:.4f}%")
        print(f"Improvement:   {improvement_vs_linear*100:+.4f}% ({pct_improvement_vs_linear:+.1f}%)")
        
        if test_metrics['test_r2'] > baseline_r2:
            print("‚úÖ Enhanced GB beats linear baseline!")
        else:
            print("‚ö†Ô∏è  Still below linear (needs more tuning)")
        
        # Compare to original GB (0.9092%)
        original_gb_r2 = 0.009092
        if test_metrics['test_r2'] > original_gb_r2:
            improvement_vs_original = test_metrics['test_r2'] - original_gb_r2
            pct_improvement_vs_original = (improvement_vs_original / original_gb_r2) * 100
            print(f"\n--- vs Original GB (0.9092%) ---")
            print(f"Improvement: {improvement_vs_original*100:+.4f}% ({pct_improvement_vs_original:+.1f}%)")
            print("‚úÖ Enhanced GB significantly better!")
        
        # ============================================
        # Feature Importance (Top 15)
        # ============================================
        print("\n--- Top 15 Feature Importance ---")
        
        if hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'feature': FEATURE_COLS,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print(importance_df.head(15).to_string(index=False))
            
            # Plot
            plt.figure(figsize=(12, 8))
            plt.barh(importance_df['feature'][:15], 
                     importance_df['importance'][:15])
            plt.xlabel('Importance')
            plt.title(f'{model_name} - Feature Importance (Top 15)')
            plt.tight_layout()
            plt.savefig(PLOTS_DIR / f'phase3_{model_name.lower()}_feature_importance.png', 
                       dpi=150, bbox_inches='tight')
            plt.close()
            print(f"‚úÖ Feature importance plot saved")
        else:
            print("‚ö†Ô∏è  Feature importance not available")
        
        # ============================================
        # Generalization Check
        # ============================================
        train_r2 = train_metrics['train_r2']
        test_r2 = test_metrics['test_r2']
        r2_gap = train_r2 - test_r2
        
        print(f"\n--- Generalization Check ---")
        print(f"Train R¬≤: {train_r2*100:.4f}%")
        print(f"Test R¬≤:  {test_r2*100:.4f}%")
        print(f"Gap:      {r2_gap*100:.4f}%")
        
        if r2_gap < 0.2:
            print("‚úÖ Excellent generalization")
        elif r2_gap < 0.5:
            print("‚úÖ Good generalization")
        elif r2_gap < 1.0:
            print("‚ö†Ô∏è  Mild overfitting")
        else:
            print("‚ö†Ô∏è  Significant overfitting")
        
        # ============================================
        # Save Results
        # ============================================
        results = {
            'phase': 'phase3_gradientboosting',
            'model': model_name,
            'horizon': TARGET_HORIZON,
            'n_features': len(FEATURE_COLS),
            'n_train': len(X_train),
            'n_test': len(X_test),
            'train_time_sec': train_time,
            'max_iter_requested': params['max_iter'],
            'n_iter_actual': actual_iterations,
            'stopped_early': stopped_early,
            'max_depth': params['max_depth'],
            'learning_rate': params['learning_rate'],
            'max_bins': params['max_bins'],
            'l2_regularization': params['l2_regularization'],
            'baseline_r2': baseline_r2,
            'improvement_over_baseline': improvement_vs_linear,
            **train_metrics,
            **val_metrics,
            **test_metrics
        }
        
        all_results.append(results)
        
        # Save model
        model_path = MODELS_DIR / f'phase3_gb.pkl'
        save_model(model, model_path)
        
        # Clean up
        del y_train_pred, y_val_pred, y_test_pred
        gc.collect()
        
        print("\n" + "-"*60)
    
    # ============================================
    # Summary
    # ============================================
    print_section("PHASE 3 SUMMARY")
    
    results_df = pd.DataFrame(all_results)
    
    print("\nModel Comparison:")
    comparison = results_df[['model', 'test_r2', 'test_directional_accuracy',
                            'improvement_over_baseline', 'train_time_sec', 'n_iter_actual']].copy()
    comparison['test_r2'] = comparison['test_r2'] * 100
    comparison['improvement_over_baseline'] = comparison['improvement_over_baseline'] * 100
    comparison = comparison.sort_values('test_r2', ascending=False)
    
    print(comparison.to_string(index=False))
    
    # Best model
    best_idx = results_df['test_r2'].idxmax()
    best_model = results_df.loc[best_idx, 'model']
    best_r2 = results_df.loc[best_idx, 'test_r2']
    best_time = results_df.loc[best_idx, 'train_time_sec']
    best_dir_acc = results_df.loc[best_idx, 'test_directional_accuracy']
    
    print(f"\nüèÜ Best Model: {best_model}")
    print(f"   Test R¬≤: {best_r2*100:.4f}%")
    print(f"   Directional Accuracy: {best_dir_acc:.2f}%")
    print(f"   Train time: {format_duration(best_time)}")
    print(f"   Improvement over Linear: {(best_r2-baseline_r2)*100:+.4f}%")
    
    # Compare to XGBoost (1.3511%)
    xgboost_r2 = 0.013511
    if best_r2 >= xgboost_r2:
        print(f"\nüéØ Enhanced GB matches/beats XGBoost ({xgboost_r2*100:.4f}%)!")
    else:
        gap = (xgboost_r2 - best_r2) * 100
        pct_gap = (gap / xgboost_r2) * 100
        print(f"\nüìä Gap to XGBoost: {gap:.4f}% ({pct_gap:.1f}%)")
    
    # Save results
    results_path = RESULTS_DIR / 'phase3_gradientboosting_results.csv'
    results_df.to_csv(results_path, index=False)
    print(f"\n‚úÖ Results saved to {results_path}")
    
    # ============================================
    # Cross-Phase Comparison
    # ============================================
    print_section("Cross-Phase Comparison")
    
    print("\n--- Performance Across Phases ---")
    print(f"Phase 1 (Linear):            {baseline_r2*100:.4f}%")
    
    # Phase 2
    try:
        phase2_results = pd.read_csv('results/phase2_randomforest_results.csv')
        phase2_best = phase2_results.loc[phase2_results['test_r2'].idxmax()]
        print(f"Phase 2 (RF Enhanced):       {phase2_best['test_r2']*100:.4f}%")
    except FileNotFoundError:
        try:
            phase2_results = pd.read_csv('results/phase2_randomforest_optimized_results.csv')
            phase2_best = phase2_results.loc[phase2_results['test_r2'].idxmax()]
            print(f"Phase 2 (RF Original):       {phase2_best['test_r2']*100:.4f}%")
        except FileNotFoundError:
            phase2_best = None
            print(f"Phase 2 (Random Forest):     Not found")
    
    print(f"Phase 3 (GB Enhanced):       {best_r2*100:.4f}%")
    print(f"Phase 4 (XGBoost):           {xgboost_r2*100:.4f}%")
    
    print("\n--- Summary ---")
    all_phases = {
        'Linear': baseline_r2,
        'GB_Enhanced': best_r2,
        'XGBoost': xgboost_r2
    }
    if phase2_best is not None:
        all_phases['RF'] = phase2_best['test_r2']
    
    best_overall = max(all_phases.items(), key=lambda x: x[1])
    print(f"üèÜ Best Overall: {best_overall[0]} with R¬≤ = {best_overall[1]*100:.4f}%")
    
    # ============================================
    # Key Improvements Explanation
    # ============================================
    print_section("Why Enhanced GB Performs Better")
    
    print("üîë Key Improvements:")
    print("1. Deeper trees (10-14 vs 4-8):")
    print("   ‚Üí Captures complex LOB structure")
    print("   ‚Üí Models multi-level order book relationships")
    print("\n2. More iterations (500-1500 vs 100-300):")
    print("   ‚Üí Better sequential learning")
    print("   ‚Üí Learns subtle residual patterns")
    print("\n3. Patient early stopping (20-50 vs 10-20):")
    print("   ‚Üí Allows full convergence")
    print("   ‚Üí Prevents premature stopping")
    print("\n4. Optimal learning rates (0.03-0.1):")
    print("   ‚Üí Balanced speed vs accuracy")
    print("   ‚Üí Better gradient descent")
    print("\n5. Less regularization:")
    print("   ‚Üí LOB patterns are real, not noise")
    print("   ‚Üí Trusts the data more")
    
    print_section("PHASE 3 COMPLETE")
    
    return results_df

# ============================================
# Run
# ============================================
if __name__ == "__main__":
    print("=" * 60)
    print("ENHANCED GRADIENT BOOSTING FOR LOB DATA")
    print("=" * 60)
    sample_display = "All samples (11.7M)" if SAMPLE_SIZE is None else f"{SAMPLE_SIZE:,}"
    print(f"Sample size: {sample_display}")
    print(f"Memory optimization: {ENABLE_MEMORY_OPTIMIZATION}")
    print(f"Algorithm: HistGradientBoostingRegressor")
    print("=" * 60)
    print()
    
    print("üöÄ ENHANCEMENTS vs Original GB:")
    print("  ‚Ä¢ Deeper trees: 10-14 (was 4-8)")
    print("  ‚Ä¢ More iterations: 500-1500 (was 100-300)")
    print("  ‚Ä¢ More patient early stopping: 20-50 (was 10-20)")
    print("  ‚Ä¢ Optimized learning rates: 0.03-0.1")
    print("  ‚Ä¢ Finer splits: min_samples_leaf 5-10 (was 10-20)")
    print("=" * 60)
    print()
    
    print("‚è±Ô∏è  Expected Training Times:")
    print("  ‚Ä¢ Fast: 10-15 minutes")
    print("  ‚Ä¢ Balanced: 15-25 minutes")
    print("  ‚Ä¢ Accurate: 25-45 minutes")
    print("  ‚Ä¢ Conservative: 30-50 minutes")
    print("=" * 60)
    print()
    
    results = train_gradient_boosting_enhanced()
    
    print("\n" + "=" * 60)
    print("TRAINING COMPLETE!")
    print("=" * 60)